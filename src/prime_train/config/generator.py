"""
Config generator for prime-rl.

Generates optimized configs based on model and hardware.
"""

from dataclasses import dataclass, field
from pathlib import Path
from typing import Optional, Any

import tomli_w
from rich.prompt import Prompt, Confirm

from prime_train.cost.presets import get_preset, HARDWARE_PRESETS
from prime_train.cost.memory import estimate_memory


@dataclass
class GeneratedConfig:
    """A generated training config."""
    model: str
    preset: str
    gpus: int
    config: dict[str, Any] = field(default_factory=dict)

    def save(self, path: Path) -> None:
        """Save config to TOML file."""
        with open(path, "wb") as f:
            tomli_w.dump(self.config, f)

    def to_toml(self) -> str:
        """Convert to TOML string."""
        import io
        buf = io.BytesIO()
        tomli_w.dump(self.config, buf)
        return buf.getvalue().decode()


def generate_config(
    model: str,
    preset: Optional[str] = None,
    gpus: int = 1,
    interactive: bool = True,
) -> GeneratedConfig:
    """
    Generate a training config.

    Args:
        model: HuggingFace model name
        preset: Hardware preset name (e.g., "h100-80gb")
        gpus: Number of GPUs
        interactive: Whether to prompt for customization

    Returns:
        GeneratedConfig object
    """
    # Get hardware preset
    hw = get_preset(preset) if preset else None

    if hw is None and interactive:
        print("Available presets:")
        for name, p in HARDWARE_PRESETS.items():
            print(f"  {name}: {p.gpu_type} ({p.vram_gb}GB)")

        preset = Prompt.ask("Select preset", choices=list(HARDWARE_PRESETS.keys()))
        hw = get_preset(preset)

    if hw is None:
        # Default to H100 settings
        from prime_train.cost.presets import HardwarePreset
        hw = HardwarePreset(
            name="default",
            gpu_type="Unknown",
            vram_gb=80,
            batch_size=64,
            max_tokens=4096,
            gpu_memory_utilization=0.85,
            rollouts_per_example=8,
            activation_checkpointing=False,
            notes="Default settings",
        )

    # Estimate memory
    memory_gb = estimate_memory(model)

    # Build config
    config = _build_config(
        model=model,
        preset=hw,
        gpus=gpus,
        memory_estimate=memory_gb,
    )

    # Interactive customization
    if interactive:
        print(f"\nGenerated config for {model} on {gpus}x {hw.gpu_type}:")
        print(f"  batch_size: {config['orchestrator'].get('batch_size', hw.batch_size)}")
        print(f"  max_tokens: {config['orchestrator']['sampling']['max_tokens']}")
        print(f"  rollouts_per_example: {config['orchestrator'].get('rollouts_per_example', hw.rollouts_per_example)}")

        if Confirm.ask("Customize?", default=False):
            config = _customize_config(config, hw)

    return GeneratedConfig(
        model=model,
        preset=hw.name,
        gpus=gpus,
        config=config,
    )


def _build_config(
    model: str,
    preset: Any,  # HardwarePreset
    gpus: int,
    memory_estimate: float,
) -> dict[str, Any]:
    """Build the base config dictionary."""
    config = {
        "config": {
            "version": "v1",
            "created": "auto-generated",
            "notes": f"Generated by prime-train for {model} on {gpus}x {preset.gpu_type}",
        },
        "orchestrator": {
            "seq_len": 8192,
            "batch_size": preset.batch_size,
            "rollouts_per_example": preset.rollouts_per_example,
            "sampling": {
                "max_tokens": preset.max_tokens,
                "temperature": 0.7,
            },
            "env": {
                "executor_backend": "local",  # Recommended for tool-calling
            },
        },
        "trainer": {
            "model": {
                "name_or_path": model,
                "seq_len": 8192,
                "dtype": "bf16",
            },
            "optimizer": {
                "type": "adamw",
                "lr": 1e-5,
            },
        },
        "inference": {
            "model": {
                "name_or_path": model,
                "dtype": "bf16",
            },
            "gpu_memory_utilization": preset.gpu_memory_utilization,
        },
    }

    # Add LoRA config (default for efficient training)
    config["trainer"]["model"]["lora"] = {
        "r": 16,
        "alpha": 32,
        "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
    }
    config["orchestrator"]["lora_name"] = "default"

    # Add activation checkpointing if needed
    if preset.activation_checkpointing:
        config["trainer"]["model"]["ac"] = {"freq": 1}

    return config


def _customize_config(config: dict[str, Any], preset: Any) -> dict[str, Any]:
    """Interactive config customization."""
    # Batch size
    batch_size = int(Prompt.ask(
        "Batch size",
        default=str(config["orchestrator"].get("batch_size", preset.batch_size)),
    ))
    config["orchestrator"]["batch_size"] = batch_size

    # Max tokens
    max_tokens = int(Prompt.ask(
        "Max tokens",
        default=str(config["orchestrator"]["sampling"]["max_tokens"]),
    ))
    config["orchestrator"]["sampling"]["max_tokens"] = max_tokens

    # Rollouts
    rollouts = int(Prompt.ask(
        "Rollouts per example",
        default=str(config["orchestrator"].get("rollouts_per_example", preset.rollouts_per_example)),
    ))
    config["orchestrator"]["rollouts_per_example"] = rollouts

    # Executor backend
    executor = Prompt.ask(
        "Executor backend",
        choices=["local", "prime"],
        default=config["orchestrator"]["env"]["executor_backend"],
    )
    config["orchestrator"]["env"]["executor_backend"] = executor

    return config
